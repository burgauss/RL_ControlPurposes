{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MountainCar_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmo0Yf1P/vjBoFKGoxMX/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burgauss/RL_ControlPurposes/blob/main/MountainCar_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpSIFSDnSgyd",
        "outputId": "7d6ffee6-ed3b-4822-a3a8-f94595ac4f37"
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rhULysnWcjS"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnHsRSY3TGuI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "ba897ee7-6259-4186-8b4c-58f97962220a"
      },
      "source": [
        "#Description of the MountainCarEnv\n",
        "'''\n",
        "    Description:\n",
        "        The agent (a car) is started at the bottom of a valley. For any given\n",
        "        state the agent may choose to accelerate to the left, right or cease\n",
        "        any acceleration.\n",
        "    Source:\n",
        "        The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
        "    Observation:\n",
        "        Type: Box(2)\n",
        "        Num    Observation               Min            Max\n",
        "        0      Car Position              -1.2           0.6\n",
        "        1      Car Velocity              -0.07          0.07\n",
        "    Actions:\n",
        "        Type: Discrete(3)\n",
        "        Num    Action\n",
        "        0      Accelerate to the Left\n",
        "        1      Don't accelerate\n",
        "        2      Accelerate to the Right\n",
        "        Note: This does not affect the amount of velocity affected by the\n",
        "        gravitational pull acting on the car.\n",
        "    Reward:\n",
        "         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
        "         on top of the mountain.\n",
        "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
        "    Starting State:\n",
        "         The position of the car is assigned a uniform random value in\n",
        "         [-0.6 , -0.4].\n",
        "         The starting velocity of the car is always assigned to 0.\n",
        "    Episode Termination:\n",
        "         The car position is more than 0.5\n",
        "         Episode length is greater than 200\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    Description:\\n        The agent (a car) is started at the bottom of a valley. For any given\\n        state the agent may choose to accelerate to the left, right or cease\\n        any acceleration.\\n    Source:\\n        The environment appeared first in Andrew Moore's PhD Thesis (1990).\\n    Observation:\\n        Type: Box(2)\\n        Num    Observation               Min            Max\\n        0      Car Position              -1.2           0.6\\n        1      Car Velocity              -0.07          0.07\\n    Actions:\\n        Type: Discrete(3)\\n        Num    Action\\n        0      Accelerate to the Left\\n        1      Don't accelerate\\n        2      Accelerate to the Right\\n        Note: This does not affect the amount of velocity affected by the\\n        gravitational pull acting on the car.\\n    Reward:\\n         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\\n         on top of the mountain.\\n         Reward of -1 is awarded if the position of the agent is less than 0.5.\\n    Starting State:\\n         The position of the car is assigned a uniform random value in\\n         [-0.6 , -0.4].\\n         The starting velocity of the car is always assigned to 0.\\n    Episode Termination:\\n         The car position is more than 0.5\\n         Episode length is greater than 200\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1RGFtKpSvrt",
        "outputId": "0e69002a-a741-453b-ec0c-2fe52fede104"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "print(\"the observation space, upper limits are \", env.observation_space.high) #upper part\n",
        "print(\"the observation space, lower limits are \", env.observation_space.low)  # lower part\n",
        "print(\"the action space is\", env.action_space.n)\n",
        "env.reset()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the observation space, upper limits are  [0.6  0.07]\n",
            "the observation space, lower limits are  [-1.2  -0.07]\n",
            "the action space is 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.53076316,  0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Itc7GAx_JM"
      },
      "source": [
        "#Hyperparameters\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 20000\n",
        "SIZE_DISCRETE = 20\n",
        "SHOW_EVERY = 1000"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWzzsO3Ayu5E"
      },
      "source": [
        "#Creation of Q_table\n",
        "\n",
        "DISCRETE_OS_SIZE = [SIZE_DISCRETE] * len(env.observation_space.high) # Matrix to help us define\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
        "\n",
        "# Size of q_table is 20x20x3, concatenation of lists\n",
        "q_table = np.random.uniform(low = -2, high = 0, size = ( DISCRETE_OS_SIZE + [env.action_space.n]))"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7NdFhzayfY1"
      },
      "source": [
        "# Get Discrete Function\n",
        "# Discrete state will return a tupple with the index of the combination of states\n",
        "# Each combination of state corresponds to a possibility of three actions according\n",
        "# to the environment\n",
        "def get_discrete_state(state):\n",
        "  discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
        "  return tuple(discrete_state.astype(np.int))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSvZMt1hTeuw"
      },
      "source": [
        "\n",
        "# When exporting data uncomment following lines\n",
        "#observation_matrix = []\n",
        "#reward_vector = []\n",
        "#done_vector = []\n",
        "for episode in range(EPISODES):\n",
        "  discrete_state = get_discrete_state(env.reset())\n",
        "  done = False\n",
        "  if episode % SHOW_EVERY == 0:\n",
        "    print(episode)\n",
        "  while not done:\n",
        "    action = np.argmax(q_table[discrete_state])\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "    print(reward, new_state, episode, action)\n",
        "    \n",
        "    if not done:\n",
        "      max_future_q = np.max(q_table[new_discrete_state])\n",
        "      current_q = q_table[discrete_state + (action, )]\n",
        "      new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "      q_table[discrete_state + (action,)] = new_q\n",
        "    elif new_state[0] >= env.goal_position:\n",
        "      q_table[discrete_state + (action,)] = 0\n",
        "      print(f\"We made it on episode {episode}\")\n",
        "      #print(new_state)\n",
        "    \n",
        "    discrete_state = new_discrete_state\n",
        "\n",
        "    # For exporting unblock following lines\n",
        "    #observation_matrix.append(new_state)\n",
        "    #reward_vector.append(reward)\n",
        "    #done_vector.append(done)\n",
        "  \n",
        "\n",
        "\n",
        "#when exporting ublock following lines\n",
        "#export_steps(observation_matrix, reward_vector, done_vector)\n",
        "\n",
        "\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kEybIpw7uc-",
        "outputId": "b0d1b0ee-d75a-4d76-e9d5-adf3f95ecfca"
      },
      "source": [
        ""
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Bxm7T-WR5D"
      },
      "source": [
        "# function for exporting data\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from google.colab import files\n",
        "\n",
        "def export_steps(observation_matrix, reward_vec, done_vec):\n",
        "    \n",
        "    observation_matrix = np.array(observation_matrix)\n",
        "    reward_vec = np.array(reward_vec).reshape((len(reward_vec),1))\n",
        "    done_vec = np.array(done_vec).reshape(len(done_vec),1)\n",
        "    step_matrix = []\n",
        "    step_matrix = np.concatenate((observation_matrix, reward_vec, done_vec), axis = 1)\n",
        "    export_array(step_matrix)\n",
        "\n",
        "\n",
        "def export_array(_list):\n",
        "\n",
        "  data = asarray(_list)\n",
        "  savetxt('data.csv', data, delimiter=',')\n",
        "  files.download('data.csv')"
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}